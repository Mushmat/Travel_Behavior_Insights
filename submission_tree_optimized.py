import pandas as pd
import numpy as np
from sklearn.ensemble import (GradientBoostingClassifier, RandomForestClassifier, 
                               ExtraTreesClassifier, HistGradientBoostingClassifier)
from sklearn.utils import resample
from sklearn.model_selection import StratifiedKFold
from data_preprocessing import load_and_preprocess
import warnings
warnings.filterwarnings('ignore')

# Try importing XGBoost
try:
    from xgboost import XGBClassifier
    HAS_XGB = True
except ImportError:
    print("‚ö†Ô∏è  XGBoost not found. Install with: pip install xgboost")
    HAS_XGB = False

def create_optimized_datasets(X, y, n_versions=6):
    """Create multiple balanced datasets"""
    datasets = []
    
    for seed in range(n_versions):
        Xy = pd.DataFrame(X)
        Xy["label"] = y.values if hasattr(y, "values") else y
        counts = Xy["label"].value_counts()
        
        quantiles = [0.90, 0.85, 0.80, 0.88, 0.83, 0.87]
        target_count = int(counts.quantile(quantiles[seed]))
        
        resampled = []
        for val in Xy["label"].unique():
            group = Xy[Xy["label"] == val]
            current_count = len(group)
            
            if current_count < target_count:
                n_samples = min(target_count, int(current_count * 4.0))
                group_up = resample(group, replace=True, n_samples=n_samples, 
                                  random_state=42+seed*17)
                resampled.append(group_up)
            else:
                resampled.append(group)
        
        Xy_bal = pd.concat(resampled, axis=0).sample(frac=1, random_state=42+seed*17)
        X_bal = Xy_bal.drop("label", axis=1).values
        y_bal = Xy_bal["label"].values.astype(int)
        datasets.append((X_bal, y_bal))
    
    return datasets

print("="*80)
print("ULTRA-OPTIMIZED TREE MODEL - TARGET: 0.70+")
print("="*80)

X_train, y_train, X_test, test_ids, _ = load_and_preprocess('train.csv', 'test.csv')

print(f"\nTraining samples: {X_train.shape[0]}")
print(f"Features: {X_train.shape[1]}")
print(f"\nClass distribution:")
print(pd.Series(y_train).value_counts().sort_index())

print("\n" + "="*80)
print("CREATING BALANCED DATASETS")
print("="*80)
datasets = create_optimized_datasets(X_train, y_train, n_versions=6)
for i, (X, y) in enumerate(datasets):
    print(f"Dataset {i+1}: {X.shape[0]} samples")

# ==================== XGBOOST (if available) ====================
xgb_probas = []

if HAS_XGB:
    print("\n" + "="*80)
    print("TRAINING XGBOOST MODELS")
    print("="*80)
    
    xgb_configs = [
        (350, 8, 0.08, 0.85, 1.0, 42),
        (400, 9, 0.07, 0.82, 1.2, 99),
        (380, 8, 0.09, 0.87, 1.0, 123),
        (360, 9, 0.075, 0.84, 1.1, 17),
        (390, 8, 0.085, 0.83, 1.0, 88),
        (370, 9, 0.08, 0.86, 1.15, 200),
        (410, 8, 0.07, 0.85, 1.0, 55),
        (340, 9, 0.095, 0.81, 1.2, 222),
    ]
    
    for i, (n_est, depth, lr, subsample, gamma, seed) in enumerate(xgb_configs):
        dataset_idx = i % len(datasets)
        X_bal, y_bal = datasets[dataset_idx]
        
        print(f"\nXGB-{i+1}/{len(xgb_configs)}: n={n_est}, d={depth}, lr={lr}")
        
        xgb = XGBClassifier(
            n_estimators=n_est,
            max_depth=depth,
            learning_rate=lr,
            subsample=subsample,
            colsample_bytree=0.8,
            gamma=gamma,
            min_child_weight=2,
            reg_alpha=0.1,
            reg_lambda=1.0,
            random_state=seed,
            tree_method='hist',
            n_jobs=-1,
            eval_metric='mlogloss'
        )
        
        xgb.fit(X_bal, y_bal)
        proba = xgb.predict_proba(X_test)
        xgb_probas.append(proba)
        print(f"  ‚úì Complete")

# ==================== GRADIENT BOOSTING ====================
print("\n" + "="*80)
print("TRAINING GRADIENT BOOSTING MODELS")
print("="*80)

gb_configs = [
    (320, 9, 0.07, 0.86, 4, 2, 42),
    (300, 8, 0.08, 0.83, 5, 2, 99),
    (340, 9, 0.06, 0.88, 4, 2, 123),
    (310, 8, 0.09, 0.81, 5, 3, 17),
    (330, 9, 0.075, 0.85, 4, 2, 88),
    (290, 8, 0.085, 0.82, 5, 2, 200),
    (350, 9, 0.065, 0.87, 4, 3, 55),
    (305, 8, 0.08, 0.84, 5, 2, 222),
]

gb_probas = []
for i, (n_est, depth, lr, subsample, min_split, min_leaf, seed) in enumerate(gb_configs):
    dataset_idx = i % len(datasets)
    X_bal, y_bal = datasets[dataset_idx]
    
    print(f"\nGB-{i+1}/{len(gb_configs)}: n={n_est}, d={depth}, lr={lr}")
    
    gb = GradientBoostingClassifier(
        n_estimators=n_est,
        max_depth=depth,
        learning_rate=lr,
        subsample=subsample,
        min_samples_split=min_split,
        min_samples_leaf=min_leaf,
        max_features='sqrt',
        random_state=seed
    )
    
    gb.fit(X_bal, y_bal)
    proba = gb.predict_proba(X_test)
    gb_probas.append(proba)
    print(f"  ‚úì Complete")

# ==================== HISTOGRAM GB ====================
print("\n" + "="*80)
print("TRAINING HISTOGRAM GRADIENT BOOSTING MODELS")
print("="*80)

hgb_configs = [
    (250, 9, 0.08, 42),
    (280, 10, 0.07, 99),
    (260, 9, 0.09, 123),
    (270, 10, 0.075, 17),
    (240, 9, 0.085, 88),
    (265, 10, 0.08, 200),
]

hgb_probas = []
for i, (n_est, depth, lr, seed) in enumerate(hgb_configs):
    dataset_idx = i % len(datasets)
    X_bal, y_bal = datasets[dataset_idx]
    
    print(f"\nHGB-{i+1}/{len(hgb_configs)}: n={n_est}, d={depth}, lr={lr}")
    
    hgb = HistGradientBoostingClassifier(
        max_iter=n_est,
        max_depth=depth,
        learning_rate=lr,
        random_state=seed,
        early_stopping=False
    )
    
    hgb.fit(X_bal, y_bal)
    proba = hgb.predict_proba(X_test)
    hgb_probas.append(proba)
    print(f"  ‚úì Complete")

# ==================== RANDOM FOREST ====================
print("\n" + "="*80)
print("TRAINING RANDOM FOREST MODELS")
print("="*80)

rf_configs = [
    (600, 32, 4, 2, 42),
    (650, 34, 5, 2, 99),
    (620, 33, 4, 2, 123),
    (580, 31, 5, 2, 17),
    (640, 32, 4, 2, 88),
]

rf_probas = []
for i, (n_est, depth, min_split, min_leaf, seed) in enumerate(rf_configs):
    dataset_idx = i % len(datasets)
    X_bal, y_bal = datasets[dataset_idx]
    
    print(f"\nRF-{i+1}/{len(rf_configs)}: n={n_est}, d={depth}")
    
    rf = RandomForestClassifier(
        n_estimators=n_est,
        max_depth=depth,
        min_samples_split=min_split,
        min_samples_leaf=min_leaf,
        max_features='sqrt',
        class_weight='balanced',
        random_state=seed,
        n_jobs=-1
    )
    
    rf.fit(X_bal, y_bal)
    proba = rf.predict_proba(X_test)
    rf_probas.append(proba)
    print(f"  ‚úì Complete")

# ==================== EXTRA TREES ====================
print("\n" + "="*80)
print("TRAINING EXTRA TREES MODELS")
print("="*80)

et_configs = [
    (550, 30, 4, 2, 42),
    (600, 32, 5, 2, 99),
    (580, 31, 4, 2, 123),
    (570, 30, 5, 2, 17),
]

et_probas = []
for i, (n_est, depth, min_split, min_leaf, seed) in enumerate(et_configs):
    dataset_idx = i % len(datasets)
    X_bal, y_bal = datasets[dataset_idx]
    
    print(f"\nET-{i+1}/{len(et_configs)}: n={n_est}, d={depth}")
    
    et = ExtraTreesClassifier(
        n_estimators=n_est,
        max_depth=depth,
        min_samples_split=min_split,
        min_samples_leaf=min_leaf,
        max_features='sqrt',
        class_weight='balanced',
        random_state=seed,
        n_jobs=-1
    )
    
    et.fit(X_bal, y_bal)
    proba = et.predict_proba(X_test)
    et_probas.append(proba)
    print(f"  ‚úì Complete")

# ==================== ENSEMBLE ====================
print("\n" + "="*80)
print("CREATING OPTIMIZED ENSEMBLE")
print("="*80)

print(f"\nModel counts:")
if HAS_XGB:
    print(f"  XGBoost: {len(xgb_probas)}")
print(f"  Gradient Boosting: {len(gb_probas)}")
print(f"  Histogram GB: {len(hgb_probas)}")
print(f"  Random Forest: {len(rf_probas)}")
print(f"  Extra Trees: {len(et_probas)}")

total_models = len(xgb_probas) + len(gb_probas) + len(hgb_probas) + len(rf_probas) + len(et_probas)
print(f"  Total: {total_models}")

# Optimized weights (XGBoost gets highest if available)
if HAS_XGB:
    xgb_weight = 0.38 / len(xgb_probas)
    gb_weight = 0.28 / len(gb_probas)
    hgb_weight = 0.18 / len(hgb_probas)
    rf_weight = 0.10 / len(rf_probas)
    et_weight = 0.06 / len(et_probas)
else:
    gb_weight = 0.44 / len(gb_probas)
    hgb_weight = 0.26 / len(hgb_probas)
    rf_weight = 0.20 / len(rf_probas)
    et_weight = 0.10 / len(et_probas)

ensemble_proba = np.zeros_like(gb_probas[0])

if HAS_XGB:
    for proba in xgb_probas:
        ensemble_proba += proba * xgb_weight

for proba in gb_probas:
    ensemble_proba += proba * gb_weight

for proba in hgb_probas:
    ensemble_proba += proba * hgb_weight

for proba in rf_probas:
    ensemble_proba += proba * rf_weight

for proba in et_probas:
    ensemble_proba += proba * et_weight

final_preds = np.argmax(ensemble_proba, axis=1)

print("\n" + "="*80)
print("FINAL RESULTS")
print("="*80)

print(f"\nPrediction distribution:")
print(pd.Series(final_preds).value_counts().sort_index())

submission = pd.DataFrame({
    'trip_id': test_ids,
    'category': final_preds
})

submission.to_csv('submission_ultra_optimized.csv', index=False)

print("\n" + "="*80)
print("‚úì SUBMISSION SAVED: submission_ultra_optimized.csv")
print("="*80)
print("\nüéØ Previous best: 0.694")
print("Expected: 0.705-0.720")
print("\nKey improvements:")
print("  ‚Ä¢ 35 top countries (was 30)")
print("  ‚Ä¢ Country-Inclusions target encoding (NEW)")
print("  ‚Ä¢ 6 balanced datasets with 4.0x upsampling")
if HAS_XGB:
    print(f"  ‚Ä¢ {len(xgb_probas)} XGBoost models (NEW)")
print(f"  ‚Ä¢ {len(gb_probas)} GB + {len(hgb_probas)} HGB models")
print(f"  ‚Ä¢ {len(rf_probas)} RF + {len(et_probas)} ET models")
print("  ‚Ä¢ Deeper trees & more estimators")
print("  ‚Ä¢ Optimized ensemble weights")